{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Project_Clustering.ipynb","version":"0.3.2","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"EmDIvfu_kNtj","colab_type":"text"},"cell_type":"markdown","source":["# 2. Clustering: Introduction\n","\n","Our objective in this Solution is to use Data Analytics to use tweets from our dataset to map words such that similar words, or words with similar sentiments are close to each other. This can help in many applications where handling text corpuses is the main problem. This method will provide a solution such that the amount of text (or data) can be easily reduced and mapped to meaning (information/knowledge) that is usable to extract and extrapolate sentiments, feelings, opinions, etc.\n","\n","\n","We will be using open source python packages and known methods like Word2Vec and K-Means clustering to achieve this task. "]},{"metadata":{"id":"0GPZqOkVdDcU","colab_type":"code","outputId":"11dae3f2-915f-4276-a57f-f6db909d0bf1","colab":{}},"cell_type":"code","source":["import pandas as pd\n","csv = 'clean_tweet.csv'\n","df = pd.read_csv(csv, index_col=0)\n","df['text'] = df['text'].astype('str')\n","df.head()\n","print(df.columns.values)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['text' 'target']\n"],"name":"stdout"}]},{"metadata":{"id":"oCU_IkjTdnFw","colab_type":"text"},"cell_type":"markdown","source":["Here, we imported the cleaned version of tweets. The dataset was cleaned earlier in the project.\n","\n","Here we see what the data looks like on which we are clustering. "]},{"metadata":{"id":"Gkhx-X_OdDch","colab_type":"code","colab":{}},"cell_type":"code","source":["data = pd.DataFrame()\n","data['text'] = df.text\n","del df"],"execution_count":0,"outputs":[]},{"metadata":{"id":"h9R4fl6LeE39","colab_type":"text"},"cell_type":"markdown","source":["Conversion to pandas for ease of handling"]},{"metadata":{"id":"58DeyqWZdDcl","colab_type":"code","colab":{}},"cell_type":"code","source":["tweets = data['text']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TA8e0SwIdDco","colab_type":"code","colab":{}},"cell_type":"code","source":["import gensim\n","from gensim.models import Word2Vec, KeyedVectors\n","# model = Word2Vec(tweets, min_count=2)\n","model = KeyedVectors.load('model.w2v')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RnOhZi0CeMaD","colab_type":"text"},"cell_type":"markdown","source":["**Gensim**: is a production-ready open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning. Gensim is implemented in Python and Cython for top performance and scalability.\n","\n","Here we use the pretrained model of Word2Vec to provide the ability to cluster on text via sentiment analysis. \n","\n","\n","---\n","\n","\n","\n","But, what is Word2Vec?\n","\n","> Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.\n","\n","> It is basically a shallow neural network that is trained to convert words to vectors of several dimensions such that similar words are close together in this space. It was created and patented at Google in 2013."]},{"metadata":{"id":"CR97qUH7dDct","colab_type":"code","outputId":"36c2b21e-d24e-4c26-e4bd-5554b4066c61","colab":{}},"cell_type":"code","source":["print(model.wv.similar_by_word('sister'))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[('brother', 0.7047790288925171), ('cousin', 0.6642760634422302), ('sis', 0.6568150520324707), ('mom', 0.5664997100830078), ('dad', 0.5584393739700317), ('bro', 0.5507373809814453), ('sisters', 0.5271422863006592), ('niece', 0.5188810229301453), ('cousins', 0.5045449733734131), ('sissy', 0.5023375749588013)]\n"],"name":"stdout"}]},{"metadata":{"id":"1EyGtgX1esrU","colab_type":"text"},"cell_type":"markdown","source":["For example, when we look at similarity with the word \"sister\" in the twitter dataset, we see the closest words as a result of the model are brother, sis, mom, dad, etc. which are very realistic to our daily language usage."]},{"metadata":{"id":"BV8BjzPndDcw","colab_type":"code","colab":{}},"cell_type":"code","source":["word_vectors = model.wv.vectors\n","n_words = word_vectors.shape[0]\n","vector_size = word_vectors.shape[1]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uYnYLwROdDcz","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.cluster import KMeans\n","n_clusters = 150\n","kmeans = KMeans(n_clusters=n_clusters, n_jobs=4)\n","idx = kmeans.fit_predict(word_vectors)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AGEz8TZvgs5_","colab_type":"text"},"cell_type":"markdown","source":["We are using these vectors as a base for clustering using K-Means clustering from the open source Scikit-Learn package.\n","\n","---\n","What is K-Means Clustering?\n","\n","> k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. \n","\n",">Hence similar words from the tweets should come into the same clusters. This helps us identify similar words and helps us automate some opinion mining problems with ease."]},{"metadata":{"id":"j4V1xfAxdDc2","colab_type":"code","outputId":"003eaebc-41e3-472a-9b8a-00e921fef2eb","colab":{}},"cell_type":"code","source":["# See if there's any semantic meaning captured in the clustering\n","words = ['tea', 'water','good',  'excellent', 'beautiful', 'bad',\n","         'ugly', 'pen', 'chocolate',\n","         'food', 'pizza', 'hungry', 'company', 'water', 'rain',\n","         'hurricane', 'brother', 'sister', 'father', 'niece',\n","         'school', 'college', 'university', 'institute', 'harvard', 'cambridge', 'oxford']\n","data = {\n","    'Word': words,\n","    'Cluster': [kmeans.predict(model.wv[word].reshape(1,-1)) for word in words]\n","}\n","# data = [[word, kmeans.predict(model.wv[word].reshape(1,-1))] for word in words]\n","print(pd.DataFrame(data).sort_values(by=['Cluster']))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["          Word Cluster\n","12     company     [3]\n","23   institute    [15]\n","22  university    [15]\n","6         ugly    [27]\n","0          tea    [39]\n","8    chocolate    [39]\n","7          pen    [55]\n","13       water    [55]\n","1        water    [55]\n","21     college    [59]\n","20      school    [59]\n","17      sister    [73]\n","19       niece    [73]\n","18      father    [73]\n","16     brother    [73]\n","14        rain    [79]\n","24     harvard    [80]\n","26      oxford    [80]\n","25   cambridge    [80]\n","15   hurricane    [91]\n","11      hungry    [94]\n","10       pizza    [94]\n","9         food    [94]\n","5          bad   [147]\n","3    excellent   [148]\n","2         good   [148]\n","4    beautiful   [148]\n"],"name":"stdout"}]},{"metadata":{"id":"Lw0gzzOriNVs","colab_type":"text"},"cell_type":"markdown","source":["## Result\n","\n","The results seen here are a projection of what this clustering solution does. We looked at some random words that may or may not similar to one another, and we were surprised at what the clustering showed us. All the university names like **Oxford**, **Harvard**, and **Cambridge** were found in the same cluster.\n","\n","The words **Hungry**, **Food**, and **Pizza** were grouped together even though all three are actually very different words, but are used in similar contexts!\n","\n","**Pen**, **Institute**, and **University** were grouped together and **School** and **College** were close by.\n","\n","These simple observations show us the accuracy of this Solution and how well this maps to daily human communication in the English language without any human intervention. This clearly represents the power of Data Analytics in todays contexts."]}]}