{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hb4Xqc-HxiNW"
   },
   "source": [
    "# 3. Using Apache Spark for Distributed Text Analysis with Python\n",
    "\n",
    "###Sentiment analysis of text with TfIdf and Logistic Regression\n",
    "\n",
    "Python is great for data science modelling, thanks to its numerous modules and packages that help achieve data science goals. But what if the data you are dealing with cannot be fit into a single machine? Maybe you can implement careful sampling to do your analysis on a single machine, but with distributed computing framework like Pyspark, you can efficiently implement the task for large data sets.\n",
    "\n",
    "Spark API is available in multiple programming languages (Scala, Java, Python and R). There are debates about how Spark performance varies depending on which language you run it on, but since we are comfortable with python, we went ahead with Pyspark\n",
    "\n",
    "---\n",
    "**Why Tf-Idf + Logistic Regression?**\n",
    "\n",
    "Through our research in this area, we learnt that TF-IDF with Logistic Regression is quite strong a combination, and showed robust performance, even as high as a Word2Vec + Convolutional Neural Network model. So in the project, we decided to use TF-IDF + Logistic Regression model with Pyspark.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_6SNRQX6yO8y"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "We are going to use PySpark and the previously cleaned dataset to perform parallel and optimised Logistic Regression to predict the sentiment of a tweet. Logistic regression works with numerical data, but we have textual data on our hands. How do we move ahead?\n",
    "\n",
    "We use Tf-Idf which stands for Term Frequency - Inverse Document Frequence. What this is will be covered later in the report when we actually use it. For now, suffice to say, it is a method to meaningfully converting text in a corpus into multi dimensional numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vhFUo18oxiNZ"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrameReader, SQLContext\n",
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IubTb6akz1fw"
   },
   "source": [
    "First step in any Apache programming is to create a SparkContext. SparkContext is needed when we want to execute operations in a cluster. SparkContext tells Spark how and where to access a cluster. It is first step to connect with Apache Cluster.\n",
    "\n",
    "We have just created an Apache spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FFimZQ9sxiNf"
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('clean_tweet.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7HrHXj80B8g"
   },
   "source": [
    "Read the cleaned tweets dataset into the spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IFgEiKRoxiNi",
    "outputId": "2d746643-7716-4ff9-d6d2-90323499236d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------+\n",
      "|_c0|                text|target|\n",
      "+---+--------------------+------+\n",
      "|  0|wants to compete ...|     0|\n",
      "|  1|it seems we are s...|     0|\n",
      "|  2|where the are my ...|     0|\n",
      "|  3|ff the meetin hat...|     0|\n",
      "|  4|        reply me pls|     4|\n",
      "+---+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ayPBKJFx0HxX"
   },
   "source": [
    "Quick visual of what the data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PzGgRxn9xiNo"
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cnJ_TrCS0Lbt"
   },
   "source": [
    "We drop all records that have \"NA\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2zTWuXbxiNr"
   },
   "outputs": [],
   "source": [
    "(train_set, val_set, test_set) = df.randomSplit([0.98, 0.01, 0.01], seed = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BWqny_Fs0ZZG"
   },
   "source": [
    "Here, we divide the entire dataset into 3 parts, each used respectively for training , validation, and testing. Here the ratios are 98% : 1% : 1% due to the generality and variety of data, we need to apportion the lion's share to the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2eHEuYl_xiNt",
    "outputId": "20cc4a8c-f55e-4f3d-a825-3b2960c93c56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "|_c0|                text|target|               words|                  tf|            features|label|\n",
      "+---+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "|  0|wants to compete ...|     0|[wants, to, compe...|(65536,[2437,6194...|(65536,[2437,6194...|  1.0|\n",
      "|  1|it seems we are s...|     0|[it, seems, we, a...|(65536,[4488,5660...|(65536,[4488,5660...|  1.0|\n",
      "|  2|where the are my ...|     0|[where, the, are,...|(65536,[2025,2458...|(65536,[2025,2458...|  1.0|\n",
      "|  3|ff the meetin hat...|     0|[ff, the, meetin,...|(65536,[7173,1412...|(65536,[7173,1412...|  1.0|\n",
      "|  4|        reply me pls|     4|    [reply, me, pls]|(65536,[2037,6933...|(65536,[2037,6933...|  0.0|\n",
      "+---+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "train_df = pipelineFit.transform(train_set)\n",
    "val_df = pipelineFit.transform(val_set)\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cL_I-Z9405vZ"
   },
   "source": [
    "Here, we perform the main Tf-Idf operation on the dataset. \n",
    "\n",
    "###So what is Tf-Idf?\n",
    "\n",
    "> Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "\n",
    ">One of the simplest ranking functions is computed by summing the tf-idf for each query term; many more sophisticated ranking functions are variants of this simple model.\n",
    "\n",
    ">Tf-idf can be successfully used for stop-words filtering in various subject fields including text summarization and classification.\n",
    "\n",
    "But how is it computed?\n",
    "\n",
    ">Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
    "\n",
    ">**TF**: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: \n",
    "\n",
    ">**TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)**\n",
    "\n",
    ">**IDF**: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n",
    "\n",
    ">**IDF(t) = log_e(Total number of documents / Number of documents with term t in it)**\n",
    "\n",
    "---\n",
    "Therefore this gives us a very simple yet elegant method of converting text into numerical data that can be used later on in the Analytics pipeline to extract useful information.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8u6tTGcjxiNx"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "lrModel = lr.fit(train_df)\n",
    "predictions = lrModel.transform(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "th_nN3Ie2dIO"
   },
   "source": [
    "Here, we use Logistic Regression to actually classify the sentiments of the text and evaluate how well our model and pipeline performed.\n",
    "\n",
    "###What is Logistic Regression?\n",
    "\n",
    "> Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).  Like all regression analyses, the logistic regression is a predictive analysis. It is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1000/1*UgYbimgPXf6XXxMy2yqRLw.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "The above image is a very apt description of what Logistic regression does, without getting too mathematical.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMeD2LmoxiN0",
    "outputId": "be9e3fdb-00fa-4be7-c617-19a05c24626f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8413855253027225"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7qRdxx5U5RcB"
   },
   "source": [
    "##Result\n",
    "On evaluation of our model, we got an ~84% accuracy prediction, which shocked us. As we ventured further into what this value is, we found out that it is actually the ROC Area Under the Curve, which is a very famous metric for evaluating classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SqWpBUm5xiN4",
    "outputId": "e41349c6-2e1d-4fda-961a-d967f2075c08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'areaUnderROC'"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KMS_n8St5vIo"
   },
   "source": [
    "As seen here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w01TjJW_xiN8",
    "outputId": "c1c6aea1-07c9-4107-be03-873ba5107eb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.781054736315921"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-a9KLimD5xeN"
   },
   "source": [
    "Therefore when we actually looked at our prediction accuracy, we got a value of ~78%.\n",
    "\n",
    "Thus after completing the whole project, our prediction accuracy was 78%, implemented in Apache Spark using Python. \n",
    "\n",
    "This taught all of us the nuances of this platform pretty well and thoroughly enjoyed the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bkXOxa__6lXr"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Spark.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
